from typing import Any, Tuple, Optional
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback
from IPython.core.debugger import set_trace as breakpoint

class LogImage2DiskCallback(Callback):

    def __init__(self, log_every: int, num_samples: int, save_dir: Path) -> None:
        """ At every `log_every` epoch, use the current G to generate a
        `num_samples` number of datapts, and write the sample to disk (`save_dir`)
        Also, use the current D to compute the avg. score on the sample of G.
        G much have a method called ".sample"""
        self.log_every = log_every
        self.num_samples = num_samples
        self.save_dir = save_dir
        if not save_dir.exists():
            save_dir.mkdir(parents=True)
            print(f"Created dir for samples: {save_dir}")

    def on_train_start(self, trainer, pl_module) -> None:
        print("Train started")

    def on_train_epoch_start(self, trainer, pl_module) -> None:
        if trainer.current_epoch < 1:
            print("Train epoch 0 started")
            print("device: ", pl_module.device)

    def on_train_batch_start(
            self,
            trainer: pl.Trainer,
            pl_module: pl.LightningModule,
            batch: Any,
            batch_idx: int,
            unused: int = 0,
    ) -> None:
        """Called when the train batch begins."""
        if trainer.current_epoch == 0 and batch_idx == 0:
            print(f"Saving real images from first batch at ep {trainer.current_epoch}, batch {batch_idx}...")
            try:
                x_real = batch['x'][:self.num_samples]  # (64, nc, h, w) tensor
            except TypeError:
                x_real = batch[0][:self.num_samples]  # (64, nc, h, w) tensor

            out_fp = self.save_dir / f'x_real_{trainer.global_step}.png'
            torchvision.utils.save_image(x_real, out_fp)
            print("Saved snapshot of real images!")

    # def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
    #     if outputs['loss'] is None:
    #         print('train loss_G is None!!')
    #         set_trace()

    def on_epoch_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:
        """Save samples generated by current G's state to `self.save_dir` """
        if (trainer.current_epoch + 1) % self.log_every == 0:
            with torch.no_grad():
                #----
                # todo: this way of returning back to original module's training state can benefit from the context manager
                was_training = pl_module.training
                pl_module.eval()
                x_gen = pl_module.sample(self.num_samples, pl_module.device)
                x_gen = x_gen.detach().numpy()
                pl_module.train(was_training)
                #----
                fp = self.save_dir / f"x_gen_epoch={trainer.current_epoch}_gstep={trainer.global_step}.png"
                torchvision.utils.save_image(x_gen, fp)


class LogScatterPlot2DiskCallback(Callback):

    def __init__(self, log_every: int, num_samples: int, save_dir: Path,
                 xlim: Optional[Tuple[float, float]]=None,
                 **plot_kwargs) -> None:
        """ At every `log_every` epoch, use the current G to generate a
        `num_samples` number of datapts, and write the sample to disk (`save_dir`)
        Also, use the current D to compute the avg. score on the sample of G.
        G much have a method called ".sample

        Args
        ----
        log_every : int; epoch interval to log the scatter plot to disk
        num_samples : int; number of samples of z's from P_z --> x's G(z)
        xlim : Tuple[float, float]; limits of the x-axis for the scatter plot
        """
        self.log_every = log_every
        self.num_samples = num_samples
        self.save_dir = save_dir
        if not save_dir.exists():
            save_dir.mkdir(parents=True)
            print(f"Created dir for samples: {save_dir}")
        self.xlim = xlim
        self.plot_kwargs = plot_kwargs

    def on_train_start(self, trainer, pl_module) -> None:
        print("Train started")

    def on_train_epoch_start(self, trainer, pl_module) -> None:
        if trainer.current_epoch < 1:
            print("Train epoch 0 started")
            print("device: ", pl_module.device)

    def on_train_batch_start(
             self,
             trainer: pl.Trainer,
             pl_module: pl.LightningModule,
             batch: Any,
             batch_idx: int,
             unused: int = 0,
    ) -> None:
        """Called when the train batch begins."""
        if trainer.current_epoch == 0 and batch_idx == 0:
            print(f"Saving real images from first batch at ep {trainer.current_epoch}, batch {batch_idx}...")
            try:
                x_real = batch['x'][:self.num_samples]  # (64, nc, h, w) tensor
            except TypeError:
                x_real = batch[0][:self.num_samples]  # (64, nc, h, w) tensor

            f, ax = plt.subplots()
            x_real = x_real.cpu().numpy()
            ax.scatter(x_real[:,0], x_real[:,1], **self.plot_kwargs)
            ax.set_title(f"x_real")
            if self.xlim is not None:
                ax.set_xlim(self.xlim)

            fp = self.save_dir / f'x_real_{trainer.global_step}.png'
            f.savefig(fp)
            print("Saved snapshot of real images!")

    # def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
    #     if outputs['loss'] is None:
    #         print('train loss_G is None!!')
    #         set_trace()
    
    def on_train_epoch_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:
        """Save samples generated by current G's state to `self.save_dir` """
        if (trainer.current_epoch + 1) % self.log_every == 0:
            with torch.no_grad():
                #----
                # todo: this way of returning back to original module's training state can benefit from the context manager
                was_training = pl_module.training
                pl_module.eval()
                x_gen = pl_module.sample(self.num_samples, pl_module.device)
                x_gen = x_gen.cpu().numpy()
                pl_module.train(was_training)
                #----
                f, ax = plt.subplots()
                ax.scatter(x_gen[:, 0], x_gen[:, 1], **self.plot_kwargs)
                ax.set_title(f"x_gen (iter: {trainer.global_step})")
                if self.xlim is not None:
                    ax.set_xlim(self.xlim)

                fp = self.save_dir / f"x_gen_epoch={trainer.current_epoch}_gstep={trainer.global_step}.png"
                f.savefig(fp)
                print(f"Saved generated samples at {trainer.current_epoch}ep: {fp}") #todo: remove


class LogImage2TBCallback(Callback):

    def __init__(self, log_every: int, num_samples: int) -> None:
        """ At every `log_every` epoch, use the current G to generate a
        `num_samples` number of datapts.
        Also, use the current D to compute the avg. score on the sample of G.
        G much have a method called ".sample"""
        self.log_every = log_every
        self.num_samples = num_samples

    def on_epoch_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:
        with torch.no_grad():

            #----- todo: use context manager
            was_training = pl_module.training
            # print('*****') #debugF
            # print(f'on_epoch_end -- module.training: {was_training} <-- should be eval?')
            pl_module.eval()
            x_gen = pl_module.sample(self.num_samples, pl_module.device)
            pl_module.train(was_training)
            # ----- end: wrapped by context manager

            grid = torchvision.utils.make_grid(x_gen)
            trainer.logger.experiment.add_image( f"x_gen", grid, trainer.global_step)  # todo: ep should be set properly


class LogScatterPlot2TBCallback(Callback):

    def __init__(
        self, 
        log_every: int, 
        num_samples: int, 
        xlim: Tuple[float, float], ylim:Tuple[float,float]=None,
                **plot_kwargs) -> None:
        """ At every `log_every` epoch, use the current G to generate a
        `num_samples` number of datapts.
        Also, use the current D to compute the avg. score on the sample of G.
        G much have a method called ".sample"""
        self.log_every = log_every
        self.num_samples = num_samples
        self.xlim = xlim
        self.ylim = ylim
        self.plot_kwargs = plot_kwargs

    def setup(self, trainer, pl_module, stage: Optional[str] = None) -> None:
    # def on_train_start(self, trainer, pl_module) -> None:

        # set additional init. attributes for this callback
        # fixed z-samples to feed to the generator
        z_samples = torch.randn((self.num_samples, pl_module.latent_dim), device=pl_module.device)
        self.sorted_zs, self.sorted_id = torch.sort(z_samples, dim=0)
        self.cmap = plt.cm.get_cmap('jet', len(self.sorted_zs))

        print("Logscatter2tb callback setup done: Set fixed zsample input")


    def on_epoch_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:

        with torch.no_grad():
            #----- todo: use context manager
            was_training = pl_module.training
            # print('*****') #debug
            # print(f'on_epoch_end -- module.training: {was_training} <-- should be eval?')
            pl_module.eval()
            x_gen = pl_module.generator(self.sorted_zs) #note: input z is sorted in increasing order
            x_gen = x_gen.cpu().numpy()

            pl_module.train(was_training)
            # ----- end: wrapped by context manager

            f, ax = plt.subplots()
            ax.scatter(x_gen[:,0], x_gen[:,1], 
                    c=self.sorted_zs.cpu().numpy(),
                    cmap=self.cmap,
                    **self.plot_kwargs)
            ax.set_title(f"x_gen (iter: {trainer.global_step})")
            if self.xlim is not None:
                ax.set_xlim(self.xlim)
            if self.ylim is not None:
                ax.set_ylim(self.ylim)
            trainer.logger.experiment.add_figure(f"x_gen", f, trainer.global_step)